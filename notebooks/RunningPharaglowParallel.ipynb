{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pharaglow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# image io and analysis\n",
    "import json\n",
    "import pims\n",
    "import trackpy as tp\n",
    "\n",
    "# plotting\n",
    "import matplotlib  as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#our packages\n",
    "from pharaglow import tracking, run, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and create binary masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# io\n",
    "fname = \"/media/scholz_la/hd2/maxi/Substrate stiffness/MV0021/*.tiff\"\n",
    "parameterfile = \"/media/scholz_la/hd2/maxi/Substrate stiffness/pharaglow_parameters_2x2.txt\"\n",
    "outfile = \"/media/scholz_la/hd2/maxi/Substrate stiffness/MV0021_{}_{}.json\"\n",
    "lawnfile = None#'/media/scholz_la/hd2/Nicolina/Nicolina/NZ0014_lawn.bmp'\n",
    "\n",
    "print('Starting pharaglow analysis...')\n",
    "rawframes = pims.open(fname)\n",
    "rawframes = rawframes\n",
    "\n",
    "print('Loading parameters from {}'.format(parameterfile.split('/')[-2:]))\n",
    "with open(parameterfile) as f:\n",
    "    param = json.load(f)\n",
    "\n",
    "if lawnfile is not None:\n",
    "    print('open and binarize lawn file')\n",
    "    lawn = pims.open(lawnfile)[0]\n",
    "    binLawn = features.findLawn(lawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(binLawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# detecting objects\n",
    "print('Binarizing images')\n",
    "masks = tracking.calculateMask(rawframes, minSize = param['minSize'], bgWindow = param['bgWindow']\n",
    "                               , thresholdWindow = param['thresholdWindow'], smooth =  param['smooth'],\n",
    "                               subtract =  param['subtract'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the thesholding worked otherwise change parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 7000\n",
    "plt.figure(figsize=(18,11))\n",
    "# plt.subplot(121)\n",
    "plt.imshow(rawframes[t])\n",
    "# plt.subplot(122)\n",
    "plt.imshow(masks[t])#[:,2500:])\n",
    "print(np.sum(masks[t]))#[:,2500:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting individual objects and tracking or use multiprocessing to speed up feature detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "nWorkers = 10\n",
    "if nWorkers ==1:\n",
    "\n",
    "    #masks = tracking.preprocess(rawframes, minSize = param['minSize'], threshold =None )\n",
    "    print('Detecting features')\n",
    "    features = tracking.runfeatureDetection(rawframes, masks, param, frameOffset = 0)\n",
    "else:\n",
    "    from multiprocessing import Pool\n",
    "    print('Detecting features')\n",
    "    def f(sl):\n",
    "        a,b = sl\n",
    "        #print(a,b)\n",
    "        return tracking.runfeatureDetection(rawframes[a:b], masks[a:b], param, frameOffset = a)\n",
    "    features = []\n",
    "    L = len(rawframes)\n",
    "    # create chunks of analysis based on how many workers we use\n",
    "    print(L)\n",
    "    chunksize = L//nWorkers//20\n",
    "    #slices = np.arange(L)\n",
    "    slices = zip((range(0,L, chunksize)), (range(chunksize,L+chunksize, chunksize)))\n",
    "    \n",
    "    p = Pool(processes = nWorkers)\n",
    "    start = time.time()\n",
    "    for k, res in enumerate(p.imap_unordered(f, slices)):\n",
    "        features.append(res)\n",
    "        if k ==10:\n",
    "            print('Expected time is approx. {} s'.format(L/chunksize*(time.time()-start)/nWorkers))\n",
    "        #print(p, time.time()-start)\n",
    "    features = pd.concat(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Save the features\n",
    "plt.plot(np.sort(features['area'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(5)\n",
    "features.info(memory_usage='deep')\n",
    "features.to_json(outfile.format('features', 'all'), orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_json(outfile.format('features', 'all'), orient='split', numpy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')\n",
    "print('Linking trajectories')\n",
    "#trajectories = tracking.linkParticles(features, param['searchRange'], param['minimalDuration'])\n",
    "#trajectories = tracking.linkParticles(features, 50, 500);\n",
    "#trajectories = tp.link_df(features, 50, memory= 30)\n",
    "#trajectories = tp.filter_stubs(trajectories,100)\n",
    "pred = tp.predict.NearestVelocityPredict()\n",
    "trajectories = pred.link_df(features, 30, memory= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trajectories['particle'].nunique())\n",
    "trajectories = tp.filter_stubs(trajectories,300)\n",
    "print(trajectories['particle'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract lawn info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def inside(x,y,binLawn):\n",
    "    return binLawn[int(y), int(x)]\n",
    "\n",
    "if lawnfile is not None:\n",
    "    trajectories['inside'] = trajectories.apply(\\\n",
    "        lambda row: pd.Series(inside(row['x'], row['y'], binLawn)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show resulting trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "tp.plot_traj(trajectories)#, superimpose=1-masks[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories.to_json(outfile.format('trajectories', 'all'), orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def parallelize_dataframe(df, func, params, n_cores):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    print([len(d) for d in df_split])\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.starmap(func, zip(df_split, np.repeat(params, len(df_split)))))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the whole pharaglow feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read data\n",
    "# trajectories = pd.read_json(outfile.format('trajectories', 'all'), orient='split', numpy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print('Extracting pharynx data')\n",
    "# trajectories = parallelize_dataframe(trajectories, run.runPharaglowOnStack, n_cores = 20, params = param)\n",
    "# print('Done tracking. Successfully tracked {} frames with {} trajectorie(s).'.format(len(rawframes), trajectories['particle'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as hdf5 format (every trajectory in a file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#trajectories.info(memory_usage='deep')\n",
    "\n",
    "for particle_index in trajectories['particle'].unique():\n",
    "    tmp = parallelize_dataframe(trajectories[trajectories.loc[:,'particle'] == particle_index], run.runPharaglowOnStack, n_cores = 10, params = param)\n",
    "    tmp.to_json(outfile.format('results', particle_index), orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
